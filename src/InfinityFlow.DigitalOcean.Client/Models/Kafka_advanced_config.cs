// <auto-generated/>
#pragma warning disable CS0618
using Microsoft.Kiota.Abstractions.Extensions;
using Microsoft.Kiota.Abstractions.Serialization;
using System.Collections.Generic;
using System.IO;
using System;
namespace InfinityFlow.DigitalOcean.Client.Models
{
    [global::System.CodeDom.Compiler.GeneratedCode("Kiota", "1.0.0")]
    #pragma warning disable CS1591
    public partial class Kafka_advanced_config : IAdditionalDataHolder, IParsable
    #pragma warning restore CS1591
    {
        /// <summary>Stores additional data not described in the OpenAPI description found when deserializing. Can be used for serialization as well.</summary>
        public IDictionary<string, object> AdditionalData { get; set; }
        /// <summary>Enable auto creation of topics</summary>
        public bool? AutoCreateTopicsEnable { get; set; }
        /// <summary>Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (&apos;gzip&apos;, &apos;snappy&apos;, &apos;lz4&apos;, &apos;zstd&apos;). It additionally accepts &apos;uncompressed&apos; which is equivalent to no compression; and &apos;producer&apos; which means retain the original compression codec set by the producer.</summary>
        public global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_compression_type? CompressionType { get; set; }
        /// <summary>Idle connections timeout: the server socket processor threads close the connections that idle for longer than this.</summary>
        public int? ConnectionsMaxIdleMs { get; set; }
        /// <summary>Replication factor for autocreated topics</summary>
        public int? DefaultReplicationFactor { get; set; }
        /// <summary>The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.</summary>
        public int? GroupInitialRebalanceDelayMs { get; set; }
        /// <summary>The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.</summary>
        public int? GroupMaxSessionTimeoutMs { get; set; }
        /// <summary>The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.</summary>
        public int? GroupMinSessionTimeoutMs { get; set; }
        /// <summary>How long are delete records retained?</summary>
        public int? LogCleanerDeleteRetentionMs { get; set; }
        /// <summary>The maximum amount of time message will remain uncompacted. Only applicable for logs that are being compacted</summary>
        public int? LogCleanerMaxCompactionLagMs { get; set; }
        /// <summary>Controls log compactor frequency. Larger value means more frequent compactions but also more space wasted for logs. Consider setting log_cleaner_max_compaction_lag_ms to enforce compactions sooner, instead of setting a very high value for this option.</summary>
        public double? LogCleanerMinCleanableRatio { get; set; }
        /// <summary>The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted.</summary>
        public int? LogCleanerMinCompactionLagMs { get; set; }
        /// <summary>The default cleanup policy for segments beyond the retention window</summary>
        public global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_log_cleanup_policy? LogCleanupPolicy { get; set; }
        /// <summary>The number of messages accumulated on a log partition before messages are flushed to disk</summary>
        public int? LogFlushIntervalMessages { get; set; }
        /// <summary>The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used</summary>
        public int? LogFlushIntervalMs { get; set; }
        /// <summary>The interval with which Kafka adds an entry to the offset index</summary>
        public int? LogIndexIntervalBytes { get; set; }
        /// <summary>The maximum size in bytes of the offset index</summary>
        public int? LogIndexSizeMaxBytes { get; set; }
        /// <summary>This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests.</summary>
        public bool? LogMessageDownconversionEnable { get; set; }
        /// <summary>The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message</summary>
        public int? LogMessageTimestampDifferenceMaxMs { get; set; }
        /// <summary>Define whether the timestamp in the message is message create time or log append time.</summary>
        public global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_log_message_timestamp_type? LogMessageTimestampType { get; set; }
        /// <summary>Controls whether to preallocate a file when creating a new segment</summary>
        public bool? LogPreallocate { get; set; }
        /// <summary>The maximum size of the log before deleting messages</summary>
        public int? LogRetentionBytes { get; set; }
        /// <summary>The number of hours to keep a log file before deleting it</summary>
        public int? LogRetentionHours { get; set; }
        /// <summary>The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.</summary>
        public int? LogRetentionMs { get; set; }
        /// <summary>The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used</summary>
        public int? LogRollJitterMs { get; set; }
        /// <summary>The maximum time before a new log segment is rolled out (in milliseconds).</summary>
        public int? LogRollMs { get; set; }
        /// <summary>The maximum size of a single log file</summary>
        public int? LogSegmentBytes { get; set; }
        /// <summary>The amount of time to wait before deleting a file from the filesystem</summary>
        public int? LogSegmentDeleteDelayMs { get; set; }
        /// <summary>The maximum number of connections allowed from each ip address (defaults to 2147483647).</summary>
        public int? MaxConnectionsPerIp { get; set; }
        /// <summary>The maximum number of incremental fetch sessions that the broker will maintain.</summary>
        public int? MaxIncrementalFetchSessionCacheSlots { get; set; }
        /// <summary>The maximum size of message that the server can receive.</summary>
        public int? MessageMaxBytes { get; set; }
        /// <summary>When a producer sets acks to &apos;all&apos; (or &apos;-1&apos;), min_insync_replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful.</summary>
        public int? MinInsyncReplicas { get; set; }
        /// <summary>Number of partitions for autocreated topics</summary>
        public int? NumPartitions { get; set; }
        /// <summary>Log retention window in minutes for offsets topic</summary>
        public int? OffsetsRetentionMinutes { get; set; }
        /// <summary>The purge interval (in number of requests) of the producer request purgatory (defaults to 1000).</summary>
        public int? ProducerPurgatoryPurgeIntervalRequests { get; set; }
        /// <summary>The number of bytes of messages to attempt to fetch for each partition (defaults to 1048576). This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made.</summary>
        public int? ReplicaFetchMaxBytes { get; set; }
        /// <summary>Maximum bytes expected for the entire fetch response (defaults to 10485760). Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum.</summary>
        public int? ReplicaFetchResponseMaxBytes { get; set; }
        /// <summary>The maximum number of bytes in a socket request (defaults to 104857600).</summary>
        public int? SocketRequestMaxBytes { get; set; }
        /// <summary>The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults to 3600000 (1 hour)).</summary>
        public int? TransactionRemoveExpiredTransactionCleanupIntervalMs { get; set; }
        /// <summary>The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (defaults to 104857600 (100 mebibytes)).</summary>
        public int? TransactionStateLogSegmentBytes { get; set; }
        /// <summary>
        /// Instantiates a new <see cref="global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config"/> and sets the default values.
        /// </summary>
        public Kafka_advanced_config()
        {
            AdditionalData = new Dictionary<string, object>();
        }
        /// <summary>
        /// Creates a new instance of the appropriate class based on discriminator value
        /// </summary>
        /// <returns>A <see cref="global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config"/></returns>
        /// <param name="parseNode">The parse node to use to read the discriminator value and create the object</param>
        public static global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config CreateFromDiscriminatorValue(IParseNode parseNode)
        {
            _ = parseNode ?? throw new ArgumentNullException(nameof(parseNode));
            return new global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config();
        }
        /// <summary>
        /// The deserialization information for the current model
        /// </summary>
        /// <returns>A IDictionary&lt;string, Action&lt;IParseNode&gt;&gt;</returns>
        public virtual IDictionary<string, Action<IParseNode>> GetFieldDeserializers()
        {
            return new Dictionary<string, Action<IParseNode>>
            {
                { "auto_create_topics_enable", n => { AutoCreateTopicsEnable = n.GetBoolValue(); } },
                { "compression_type", n => { CompressionType = n.GetEnumValue<global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_compression_type>(); } },
                { "connections_max_idle_ms", n => { ConnectionsMaxIdleMs = n.GetIntValue(); } },
                { "default_replication_factor", n => { DefaultReplicationFactor = n.GetIntValue(); } },
                { "group_initial_rebalance_delay_ms", n => { GroupInitialRebalanceDelayMs = n.GetIntValue(); } },
                { "group_max_session_timeout_ms", n => { GroupMaxSessionTimeoutMs = n.GetIntValue(); } },
                { "group_min_session_timeout_ms", n => { GroupMinSessionTimeoutMs = n.GetIntValue(); } },
                { "log_cleaner_delete_retention_ms", n => { LogCleanerDeleteRetentionMs = n.GetIntValue(); } },
                { "log_cleaner_max_compaction_lag_ms", n => { LogCleanerMaxCompactionLagMs = n.GetIntValue(); } },
                { "log_cleaner_min_cleanable_ratio", n => { LogCleanerMinCleanableRatio = n.GetDoubleValue(); } },
                { "log_cleaner_min_compaction_lag_ms", n => { LogCleanerMinCompactionLagMs = n.GetIntValue(); } },
                { "log_cleanup_policy", n => { LogCleanupPolicy = n.GetEnumValue<global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_log_cleanup_policy>(); } },
                { "log_flush_interval_messages", n => { LogFlushIntervalMessages = n.GetIntValue(); } },
                { "log_flush_interval_ms", n => { LogFlushIntervalMs = n.GetIntValue(); } },
                { "log_index_interval_bytes", n => { LogIndexIntervalBytes = n.GetIntValue(); } },
                { "log_index_size_max_bytes", n => { LogIndexSizeMaxBytes = n.GetIntValue(); } },
                { "log_message_downconversion_enable", n => { LogMessageDownconversionEnable = n.GetBoolValue(); } },
                { "log_message_timestamp_difference_max_ms", n => { LogMessageTimestampDifferenceMaxMs = n.GetIntValue(); } },
                { "log_message_timestamp_type", n => { LogMessageTimestampType = n.GetEnumValue<global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_log_message_timestamp_type>(); } },
                { "log_preallocate", n => { LogPreallocate = n.GetBoolValue(); } },
                { "log_retention_bytes", n => { LogRetentionBytes = n.GetIntValue(); } },
                { "log_retention_hours", n => { LogRetentionHours = n.GetIntValue(); } },
                { "log_retention_ms", n => { LogRetentionMs = n.GetIntValue(); } },
                { "log_roll_jitter_ms", n => { LogRollJitterMs = n.GetIntValue(); } },
                { "log_roll_ms", n => { LogRollMs = n.GetIntValue(); } },
                { "log_segment_bytes", n => { LogSegmentBytes = n.GetIntValue(); } },
                { "log_segment_delete_delay_ms", n => { LogSegmentDeleteDelayMs = n.GetIntValue(); } },
                { "max_connections_per_ip", n => { MaxConnectionsPerIp = n.GetIntValue(); } },
                { "max_incremental_fetch_session_cache_slots", n => { MaxIncrementalFetchSessionCacheSlots = n.GetIntValue(); } },
                { "message_max_bytes", n => { MessageMaxBytes = n.GetIntValue(); } },
                { "min_insync_replicas", n => { MinInsyncReplicas = n.GetIntValue(); } },
                { "num_partitions", n => { NumPartitions = n.GetIntValue(); } },
                { "offsets_retention_minutes", n => { OffsetsRetentionMinutes = n.GetIntValue(); } },
                { "producer_purgatory_purge_interval_requests", n => { ProducerPurgatoryPurgeIntervalRequests = n.GetIntValue(); } },
                { "replica_fetch_max_bytes", n => { ReplicaFetchMaxBytes = n.GetIntValue(); } },
                { "replica_fetch_response_max_bytes", n => { ReplicaFetchResponseMaxBytes = n.GetIntValue(); } },
                { "socket_request_max_bytes", n => { SocketRequestMaxBytes = n.GetIntValue(); } },
                { "transaction_remove_expired_transaction_cleanup_interval_ms", n => { TransactionRemoveExpiredTransactionCleanupIntervalMs = n.GetIntValue(); } },
                { "transaction_state_log_segment_bytes", n => { TransactionStateLogSegmentBytes = n.GetIntValue(); } },
            };
        }
        /// <summary>
        /// Serializes information the current object
        /// </summary>
        /// <param name="writer">Serialization writer to use to serialize this model</param>
        public virtual void Serialize(ISerializationWriter writer)
        {
            _ = writer ?? throw new ArgumentNullException(nameof(writer));
            writer.WriteBoolValue("auto_create_topics_enable", AutoCreateTopicsEnable);
            writer.WriteEnumValue<global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_compression_type>("compression_type", CompressionType);
            writer.WriteIntValue("connections_max_idle_ms", ConnectionsMaxIdleMs);
            writer.WriteIntValue("default_replication_factor", DefaultReplicationFactor);
            writer.WriteIntValue("group_initial_rebalance_delay_ms", GroupInitialRebalanceDelayMs);
            writer.WriteIntValue("group_max_session_timeout_ms", GroupMaxSessionTimeoutMs);
            writer.WriteIntValue("group_min_session_timeout_ms", GroupMinSessionTimeoutMs);
            writer.WriteIntValue("log_cleaner_delete_retention_ms", LogCleanerDeleteRetentionMs);
            writer.WriteIntValue("log_cleaner_max_compaction_lag_ms", LogCleanerMaxCompactionLagMs);
            writer.WriteDoubleValue("log_cleaner_min_cleanable_ratio", LogCleanerMinCleanableRatio);
            writer.WriteIntValue("log_cleaner_min_compaction_lag_ms", LogCleanerMinCompactionLagMs);
            writer.WriteEnumValue<global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_log_cleanup_policy>("log_cleanup_policy", LogCleanupPolicy);
            writer.WriteIntValue("log_flush_interval_messages", LogFlushIntervalMessages);
            writer.WriteIntValue("log_flush_interval_ms", LogFlushIntervalMs);
            writer.WriteIntValue("log_index_interval_bytes", LogIndexIntervalBytes);
            writer.WriteIntValue("log_index_size_max_bytes", LogIndexSizeMaxBytes);
            writer.WriteBoolValue("log_message_downconversion_enable", LogMessageDownconversionEnable);
            writer.WriteIntValue("log_message_timestamp_difference_max_ms", LogMessageTimestampDifferenceMaxMs);
            writer.WriteEnumValue<global::InfinityFlow.DigitalOcean.Client.Models.Kafka_advanced_config_log_message_timestamp_type>("log_message_timestamp_type", LogMessageTimestampType);
            writer.WriteBoolValue("log_preallocate", LogPreallocate);
            writer.WriteIntValue("log_retention_bytes", LogRetentionBytes);
            writer.WriteIntValue("log_retention_hours", LogRetentionHours);
            writer.WriteIntValue("log_retention_ms", LogRetentionMs);
            writer.WriteIntValue("log_roll_jitter_ms", LogRollJitterMs);
            writer.WriteIntValue("log_roll_ms", LogRollMs);
            writer.WriteIntValue("log_segment_bytes", LogSegmentBytes);
            writer.WriteIntValue("log_segment_delete_delay_ms", LogSegmentDeleteDelayMs);
            writer.WriteIntValue("max_connections_per_ip", MaxConnectionsPerIp);
            writer.WriteIntValue("max_incremental_fetch_session_cache_slots", MaxIncrementalFetchSessionCacheSlots);
            writer.WriteIntValue("message_max_bytes", MessageMaxBytes);
            writer.WriteIntValue("min_insync_replicas", MinInsyncReplicas);
            writer.WriteIntValue("num_partitions", NumPartitions);
            writer.WriteIntValue("offsets_retention_minutes", OffsetsRetentionMinutes);
            writer.WriteIntValue("producer_purgatory_purge_interval_requests", ProducerPurgatoryPurgeIntervalRequests);
            writer.WriteIntValue("replica_fetch_max_bytes", ReplicaFetchMaxBytes);
            writer.WriteIntValue("replica_fetch_response_max_bytes", ReplicaFetchResponseMaxBytes);
            writer.WriteIntValue("socket_request_max_bytes", SocketRequestMaxBytes);
            writer.WriteIntValue("transaction_remove_expired_transaction_cleanup_interval_ms", TransactionRemoveExpiredTransactionCleanupIntervalMs);
            writer.WriteIntValue("transaction_state_log_segment_bytes", TransactionStateLogSegmentBytes);
            writer.WriteAdditionalData(AdditionalData);
        }
    }
}
#pragma warning restore CS0618
